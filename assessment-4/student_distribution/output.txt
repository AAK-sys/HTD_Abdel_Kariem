============================= test session starts =============================
platform win32 -- Python 3.12.6, pytest-8.4.0, pluggy-1.6.0 -- C:\Python312\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\akmk2\Desktop\data literacy\assessments\HTD_Abdel_Kariem\assessment-4\student_distribution
plugins: anyio-4.9.0, Faker-37.4.0, cov-6.2.1
collecting ... collected 46 items

tests/test_e2e.py::test_etl_pipeline_e2e PASSED                          [  2%]
tests/test_e2e.py::test_etl_pipeline_error_branches FAILED               [  4%]
tests/test_e2e.py::test_verify_mongodb PASSED                            [  6%]
tests/test_integration.py::test_extract_and_transform_book_series PASSED [  8%]
tests/test_integration.py::test_extract_and_transform_author_collaborations PASSED [ 10%]
tests/test_integration.py::test_schema_variation_handling PASSED         [ 13%]
tests/test_integration.py::test_error_scenario_source_unavailable PASSED [ 15%]
tests/test_integration.py::test_error_scenario_empty_data PASSED         [ 17%]
tests/test_integration.py::test_error_scenario_corrupted_data PASSED     [ 19%]
tests/test_unit.py::test_extract_csv_book_catalog PASSED                 [ 21%]
tests/test_unit.py::test_extract_json_author_profiles PASSED             [ 23%]
tests/test_unit.py::test_extract_mongodb_customers PASSED                [ 26%]
tests/test_unit.py::test_extract_sqlserver_table PASSED                  [ 28%]
tests/test_unit.py::test_clean_dates PASSED                              [ 30%]
tests/test_unit.py::test_clean_emails PASSED                             [ 32%]
tests/test_unit.py::test_clean_phone_numbers PASSED                      [ 34%]
tests/test_unit.py::test_clean_numerics PASSED                           [ 36%]
tests/test_unit.py::test_clean_text PASSED                               [ 39%]
tests/test_unit.py::test_remove_duplicates PASSED                        [ 41%]
tests/test_unit.py::test_handle_missing_values PASSED                    [ 43%]
tests/test_unit.py::test_handle_missing_values_unknown_strategy PASSED   [ 45%]
tests/test_unit.py::test_validate_field_level PASSED                     [ 47%]
tests/test_unit.py::test_validate_list_length PASSED                     [ 50%]
tests/test_unit.py::test_validate_list_length_nonlist PASSED             [ 52%]
tests/test_unit.py::test_generate_quality_report PASSED                  [ 54%]
tests/test_unit.py::test_generate_quality_report_empty PASSED            [ 56%]
tests/test_unit.py::test_generate_quality_report_bad_tuple_length PASSED [ 58%]
tests/test_unit.py::test_transform_book_series PASSED                    [ 60%]
tests/test_unit.py::test_transform_book_series_missing_column PASSED     [ 63%]
tests/test_unit.py::test_transform_author_collaborations PASSED          [ 65%]
tests/test_unit.py::test_transform_author_collaborations_missing_column PASSED [ 67%]
tests/test_unit.py::test_transform_reading_history PASSED                [ 69%]
tests/test_unit.py::test_transform_reading_history_missing_column PASSED [ 71%]
tests/test_unit.py::test_transform_book_recommendations PASSED           [ 73%]
tests/test_unit.py::test_transform_genre_preferences PASSED              [ 76%]
tests/test_unit.py::test_transform_genre_preferences_missing_column PASSED [ 78%]
tests/test_unit.py::test_load_dimension_table PASSED                     [ 80%]
tests/test_unit.py::test_load_fact_table PASSED                          [ 82%]
tests/test_unit.py::test_etl_pipeline_main_runs FAILED                   [ 84%]
tests/test_unit.py::test_validate_field_level_empty_df PASSED            [ 86%]
tests/test_unit.py::test_validate_field_level_missing_field PASSED       [ 89%]
tests/test_unit.py::test_generate_quality_report_none PASSED             [ 91%]
tests/test_unit.py::test_generate_quality_report_unexpected PASSED       [ 93%]
tests/test_unit.py::test_validate_list_length_empty_df PASSED            [ 95%]
tests/test_unit.py::test_generate_quality_report_malformed_tuple PASSED  [ 97%]
tests/test_unit.py::test_validate_field_level_no_rules PASSED            [100%]

================================== FAILURES ===================================
______________________ test_etl_pipeline_error_branches _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000011172BB5940>

    def test_etl_pipeline_error_branches(monkeypatch):
        import etl.etl_pipeline as pipeline
        import pandas as pd
        # Simulate error in extraction
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: (_ for _ in ()).throw(Exception("extract error")))
        with pytest.raises(Exception, match="extract error"):
            pipeline.main()
        # Simulate error in cleaning
        dummy_df = pd.DataFrame({"pub_date": []})
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: dummy_df)
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda *a, **kw: (_ for _ in ()).throw(Exception("clean error")))
        with pytest.raises(Exception, match="clean error"):
            pipeline.main()
        # Simulate error in validation
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda *a, **kw: dummy_df)
        monkeypatch.setattr(pipeline.data_quality, "validate_field_level", lambda *a, **kw: (_ for _ in ()).throw(Exception("validate error")))
        with pytest.raises(Exception, match="validate error"):
            pipeline.main()
        # Simulate error in transformation
        monkeypatch.setattr(pipeline.data_quality, "validate_field_level", lambda *a, **kw: [])
        monkeypatch.setattr(pipeline.transformers, "transform_book_series", lambda *a, **kw: (_ for _ in ()).throw(Exception("transform error")))
        with pytest.raises(Exception, match="transform error"):
            pipeline.main()
        # Simulate error in SQL loading
        # Patch all previous steps to return valid DataFrames
        valid_books = pd.DataFrame({"title": ["Book1"], "pub_date": ["2020-01-01"], "author": ["A"]})
        valid_authors = pd.DataFrame({"email": ["a@example.com"]})
        valid_customers = pd.DataFrame({"email": ["c@example.com"]})
        valid_orders = pd.DataFrame({"order_id": [1]})
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: valid_books)
        monkeypatch.setattr(pipeline.extractors, "extract_json_author_profiles", lambda *a, **kw: valid_authors)
        monkeypatch.setattr(pipeline.extractors, "extract_mongodb_customers", lambda *a, **kw: valid_customers)
        monkeypatch.setattr(pipeline.extractors, "extract_sqlserver_table", lambda *a, **kw: valid_orders)
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda df, field: df)
        monkeypatch.setattr(pipeline.cleaning, "clean_dates", lambda df, field: df)
        monkeypatch.setattr(pipeline.cleaning, "clean_emails", lambda df, field: df)
        monkeypatch.setattr(pipeline.transformers, "transform_book_series", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_author_collaborations", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_reading_history", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_genre_preferences", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_book_recommendations", lambda df1, df2: df1)
        monkeypatch.setattr(pipeline.loaders, "load_dimension_table", lambda *a, **kw: (_ for _ in ()).throw(Exception("sql load error")))
        with pytest.raises(Exception, match="sql load error"):
>           pipeline.main()

tests\test_e2e.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
etl\etl_pipeline.py:58: in main
    loaders.create_dim_dates(start_date='1950-01-01', end_date='2025-01-01', sql_conn_str=DATABASE_CONFIG['sql_server_dw'])
etl\loaders.py:33: in create_dim_dates
    dim_dates['date_key'] = dim_dates['date'].dt.strftime('%Y-%m-%d').astype(int)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\generic.py:6662: in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\managers.py:430: in astype
    return self.apply(
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\managers.py:363: in apply
    applied = getattr(b, f)(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\blocks.py:784: in astype
    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:237: in astype_array_safe
    new_values = astype_array(values, dtype, copy=copy)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:182: in astype_array
    values = _astype_nansafe(values, dtype, copy=copy)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

arr = array(['1950-01-01', '1950-01-02', '1950-01-03', ..., '2024-12-30',
       '2024-12-31', '2025-01-01'], shape=(27395,), dtype=object)
dtype = dtype('int64'), copy = True, skipna = False

    def _astype_nansafe(
        arr: np.ndarray, dtype: DtypeObj, copy: bool = True, skipna: bool = False
    ) -> ArrayLike:
        """
        Cast the elements of an array to a given dtype a nan-safe manner.
    
        Parameters
        ----------
        arr : ndarray
        dtype : np.dtype or ExtensionDtype
        copy : bool, default True
            If False, a view will be attempted but may fail, if
            e.g. the item sizes don't align.
        skipna: bool, default False
            Whether or not we should skip NaN when casting as a string-type.
    
        Raises
        ------
        ValueError
            The dtype was a datetime64/timedelta64 dtype, but it had no unit.
        """
    
        # dispatch on extension dtype if needed
        if isinstance(dtype, ExtensionDtype):
            return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)
    
        elif not isinstance(dtype, np.dtype):  # pragma: no cover
            raise ValueError("dtype must be np.dtype or ExtensionDtype")
    
        if arr.dtype.kind in "mM":
            from pandas.core.construction import ensure_wrapped_if_datetimelike
    
            arr = ensure_wrapped_if_datetimelike(arr)
            res = arr.astype(dtype, copy=copy)
            return np.asarray(res)
    
        if issubclass(dtype.type, str):
            shape = arr.shape
            if arr.ndim > 1:
                arr = arr.ravel()
            return lib.ensure_string_array(
                arr, skipna=skipna, convert_na_value=False
            ).reshape(shape)
    
        elif np.issubdtype(arr.dtype, np.floating) and dtype.kind in "iu":
            return _astype_float_to_int_nansafe(arr, dtype, copy)
    
        elif arr.dtype == object:
            # if we have a datetime/timedelta array of objects
            # then coerce to datetime64[ns] and use DatetimeArray.astype
    
            if lib.is_np_dtype(dtype, "M"):
                from pandas.core.arrays import DatetimeArray
    
                dta = DatetimeArray._from_sequence(arr, dtype=dtype)
                return dta._ndarray
    
            elif lib.is_np_dtype(dtype, "m"):
                from pandas.core.construction import ensure_wrapped_if_datetimelike
    
                # bc we know arr.dtype == object, this is equivalent to
                #  `np.asarray(to_timedelta(arr))`, but using a lower-level API that
                #  does not require a circular import.
                tdvals = array_to_timedelta64(arr).view("m8[ns]")
    
                tda = ensure_wrapped_if_datetimelike(tdvals)
                return tda.astype(dtype, copy=False)._ndarray
    
        if dtype.name in ("datetime64", "timedelta64"):
            msg = (
                f"The '{dtype.name}' dtype has no unit. Please pass in "
                f"'{dtype.name}[ns]' instead."
            )
            raise ValueError(msg)
    
        if copy or arr.dtype == object or dtype == object:
            # Explicit copy, or required since NumPy can't view from / to object.
>           return arr.astype(dtype, copy=True)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           ValueError: invalid literal for int() with base 10: '1950-01-01'

C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:133: ValueError

During handling of the above exception, another exception occurred:

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x0000011172BB5940>

    def test_etl_pipeline_error_branches(monkeypatch):
        import etl.etl_pipeline as pipeline
        import pandas as pd
        # Simulate error in extraction
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: (_ for _ in ()).throw(Exception("extract error")))
        with pytest.raises(Exception, match="extract error"):
            pipeline.main()
        # Simulate error in cleaning
        dummy_df = pd.DataFrame({"pub_date": []})
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: dummy_df)
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda *a, **kw: (_ for _ in ()).throw(Exception("clean error")))
        with pytest.raises(Exception, match="clean error"):
            pipeline.main()
        # Simulate error in validation
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda *a, **kw: dummy_df)
        monkeypatch.setattr(pipeline.data_quality, "validate_field_level", lambda *a, **kw: (_ for _ in ()).throw(Exception("validate error")))
        with pytest.raises(Exception, match="validate error"):
            pipeline.main()
        # Simulate error in transformation
        monkeypatch.setattr(pipeline.data_quality, "validate_field_level", lambda *a, **kw: [])
        monkeypatch.setattr(pipeline.transformers, "transform_book_series", lambda *a, **kw: (_ for _ in ()).throw(Exception("transform error")))
        with pytest.raises(Exception, match="transform error"):
            pipeline.main()
        # Simulate error in SQL loading
        # Patch all previous steps to return valid DataFrames
        valid_books = pd.DataFrame({"title": ["Book1"], "pub_date": ["2020-01-01"], "author": ["A"]})
        valid_authors = pd.DataFrame({"email": ["a@example.com"]})
        valid_customers = pd.DataFrame({"email": ["c@example.com"]})
        valid_orders = pd.DataFrame({"order_id": [1]})
        monkeypatch.setattr(pipeline.extractors, "extract_csv_book_catalog", lambda *a, **kw: valid_books)
        monkeypatch.setattr(pipeline.extractors, "extract_json_author_profiles", lambda *a, **kw: valid_authors)
        monkeypatch.setattr(pipeline.extractors, "extract_mongodb_customers", lambda *a, **kw: valid_customers)
        monkeypatch.setattr(pipeline.extractors, "extract_sqlserver_table", lambda *a, **kw: valid_orders)
        monkeypatch.setattr(pipeline.cleaning, "clean_text", lambda df, field: df)
        monkeypatch.setattr(pipeline.cleaning, "clean_dates", lambda df, field: df)
        monkeypatch.setattr(pipeline.cleaning, "clean_emails", lambda df, field: df)
        monkeypatch.setattr(pipeline.transformers, "transform_book_series", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_author_collaborations", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_reading_history", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_genre_preferences", lambda df: df)
        monkeypatch.setattr(pipeline.transformers, "transform_book_recommendations", lambda df1, df2: df1)
        monkeypatch.setattr(pipeline.loaders, "load_dimension_table", lambda *a, **kw: (_ for _ in ()).throw(Exception("sql load error")))
>       with pytest.raises(Exception, match="sql load error"):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: Regex pattern did not match.
E        Regex: 'sql load error'
E        Input: "invalid literal for int() with base 10: '1950-01-01'"

tests\test_e2e.py:81: AssertionError
---------------------------- Captured stdout call -----------------------------
     order_id  customer_id          book_isbn  order_date  quantity  price
0           1          121  978-1-875816-72-9  2007-12-15         3  75.77
1           2           45  978-1-76438-891-7  2008-01-11         5  44.66
2           3          117  978-0-302-72812-3  2014-01-06         2  51.83
3           4           30  978-1-161-39460-3  2020-02-30         2  88.92
4           5          419  978-1-365-81015-2  2016-04-16         5  39.35
..        ...          ...                ...         ...       ...    ...
835        24          189  978-0-415-18911-8  1988-06-15         2  41.32
836       693          243  978-1-67539-952-1  1987-12-25         1  70.35
837       399          126  978-0-611-54292-2  1984-06-08         3  92.24
838       797          134            INVALID  1998-10-12         1  48.08
839       742           39  978-1-900532-70-9  2012-03-01         2  23.48

[840 rows x 6 columns]
   order_id
0         1
_________________________ test_etl_pipeline_main_runs _________________________

    def test_etl_pipeline_main_runs():
        # Should not raise error
>       etl_pipeline.main()

tests\test_unit.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
etl\etl_pipeline.py:58: in main
    loaders.create_dim_dates(start_date='1950-01-01', end_date='2025-01-01', sql_conn_str=DATABASE_CONFIG['sql_server_dw'])
etl\loaders.py:33: in create_dim_dates
    dim_dates['date_key'] = dim_dates['date'].dt.strftime('%Y-%m-%d').astype(int)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\generic.py:6662: in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\managers.py:430: in astype
    return self.apply(
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\managers.py:363: in apply
    applied = getattr(b, f)(**kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\internals\blocks.py:784: in astype
    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:237: in astype_array_safe
    new_values = astype_array(values, dtype, copy=copy)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:182: in astype_array
    values = _astype_nansafe(values, dtype, copy=copy)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

arr = array(['1950-01-01', '1950-01-02', '1950-01-03', ..., '2024-12-30',
       '2024-12-31', '2025-01-01'], shape=(27395,), dtype=object)
dtype = dtype('int64'), copy = True, skipna = False

    def _astype_nansafe(
        arr: np.ndarray, dtype: DtypeObj, copy: bool = True, skipna: bool = False
    ) -> ArrayLike:
        """
        Cast the elements of an array to a given dtype a nan-safe manner.
    
        Parameters
        ----------
        arr : ndarray
        dtype : np.dtype or ExtensionDtype
        copy : bool, default True
            If False, a view will be attempted but may fail, if
            e.g. the item sizes don't align.
        skipna: bool, default False
            Whether or not we should skip NaN when casting as a string-type.
    
        Raises
        ------
        ValueError
            The dtype was a datetime64/timedelta64 dtype, but it had no unit.
        """
    
        # dispatch on extension dtype if needed
        if isinstance(dtype, ExtensionDtype):
            return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)
    
        elif not isinstance(dtype, np.dtype):  # pragma: no cover
            raise ValueError("dtype must be np.dtype or ExtensionDtype")
    
        if arr.dtype.kind in "mM":
            from pandas.core.construction import ensure_wrapped_if_datetimelike
    
            arr = ensure_wrapped_if_datetimelike(arr)
            res = arr.astype(dtype, copy=copy)
            return np.asarray(res)
    
        if issubclass(dtype.type, str):
            shape = arr.shape
            if arr.ndim > 1:
                arr = arr.ravel()
            return lib.ensure_string_array(
                arr, skipna=skipna, convert_na_value=False
            ).reshape(shape)
    
        elif np.issubdtype(arr.dtype, np.floating) and dtype.kind in "iu":
            return _astype_float_to_int_nansafe(arr, dtype, copy)
    
        elif arr.dtype == object:
            # if we have a datetime/timedelta array of objects
            # then coerce to datetime64[ns] and use DatetimeArray.astype
    
            if lib.is_np_dtype(dtype, "M"):
                from pandas.core.arrays import DatetimeArray
    
                dta = DatetimeArray._from_sequence(arr, dtype=dtype)
                return dta._ndarray
    
            elif lib.is_np_dtype(dtype, "m"):
                from pandas.core.construction import ensure_wrapped_if_datetimelike
    
                # bc we know arr.dtype == object, this is equivalent to
                #  `np.asarray(to_timedelta(arr))`, but using a lower-level API that
                #  does not require a circular import.
                tdvals = array_to_timedelta64(arr).view("m8[ns]")
    
                tda = ensure_wrapped_if_datetimelike(tdvals)
                return tda.astype(dtype, copy=False)._ndarray
    
        if dtype.name in ("datetime64", "timedelta64"):
            msg = (
                f"The '{dtype.name}' dtype has no unit. Please pass in "
                f"'{dtype.name}[ns]' instead."
            )
            raise ValueError(msg)
    
        if copy or arr.dtype == object or dtype == object:
            # Explicit copy, or required since NumPy can't view from / to object.
>           return arr.astype(dtype, copy=True)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           ValueError: invalid literal for int() with base 10: '1950-01-01'

C:\Users\akmk2\AppData\Roaming\Python\Python312\site-packages\pandas\core\dtypes\astype.py:133: ValueError
---------------------------- Captured stdout call -----------------------------
     order_id  customer_id          book_isbn  order_date  quantity  price
0           1          121  978-1-875816-72-9  2007-12-15         3  75.77
1           2           45  978-1-76438-891-7  2008-01-11         5  44.66
2           3          117  978-0-302-72812-3  2014-01-06         2  51.83
3           4           30  978-1-161-39460-3  2020-02-30         2  88.92
4           5          419  978-1-365-81015-2  2016-04-16         5  39.35
..        ...          ...                ...         ...       ...    ...
835        24          189  978-0-415-18911-8  1988-06-15         2  41.32
836       693          243  978-1-67539-952-1  1987-12-25         1  70.35
837       399          126  978-0-611-54292-2  1984-06-08         3  92.24
838       797          134            INVALID  1998-10-12         1  48.08
839       742           39  978-1-900532-70-9  2012-03-01         2  23.48

[840 rows x 6 columns]
=========================== short test summary info ===========================
FAILED tests/test_e2e.py::test_etl_pipeline_error_branches - AssertionError: ...
FAILED tests/test_unit.py::test_etl_pipeline_main_runs - ValueError: invalid ...
======================== 2 failed, 44 passed in 1.13s =========================
